\chapter{Algorithms} \label{chap:algorithms}

This chapter gives an overview over the available algorithms in {\ViennaCL}.
The focus of {\ViennaCL} is on iterative solvers, for which {\ViennaCL} provides a generic implementation that allows the use of the same code on the CPU (either using \ublas, Eigen, MTL4 or \OpenCL) and on the GPU (using \OpenCL).

\section{Direct Solvers} \label{sec:direct-solvers}
{\ViennaCLversion} provides triangular solvers and LU factorization without pivoting for the solution of dense linear systems. The interface is similar to that of {\ublas}

\begin{lstlisting}
  using namespace viennacl::linalg;  //to keep solver calls short
  viennacl::matrix<float>  vcl_matrix;
  viennacl::vector<float>  vcl_rhs;
  viennacl::vector<float>  vcl_result;

  /* Set up matrix and vectors here */

  //solution of an upper triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs, upper_tag());
  //solution of a lower triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs, lower_tag());

  //solution of a full system right into the load vector vcl_rhs:
  lu_factorize(vcl_matrix);
  lu_substitute(vcl_matrix, vcl_rhs);
\end{lstlisting}
In {\ViennaCLminorversion} there is no pivoting included in the LU factorization
process, hence the computation may break down or yield results with poor
accuracy. However, for certain classes of matrices (like diagonal dominant
matrices) good results can be obtained without pivoting.

It is also possible to solve for multiple right hand sides:
\begin{lstlisting}
  using namespace viennacl::linalg;  //to keep solver calls short
  viennacl::matrix<float>  vcl_matrix;
  viennacl::matrix<float>  vcl_rhs_matrix;
  viennacl::matrix<float>  vcl_result;

  /* Set up matrices here */

  //solution of an upper triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs_matrix, upper_tag());

  //solution of a lower triangular system:
  vcl_result = solve(vcl_matrix, vcl_rhs_matrix, lower_tag());
\end{lstlisting}


\section{Iterative Solvers} \label{sec:iterative-solvers}
{\ViennaCL} provides different iterative solvers for various classes of
matrices, listed in Tab.~\ref{tab:linear-solvers}. Unlike direct solvers, the
convergence of iterative solvers relies on certain properties of the system
matrix. Keep in mind that an iterative solver may fail to converge, especially
if the matrix is ill conditioned or a wrong solver is chosen.

\TIP{For full details on linear solver calls, refer to the reference
documentation located in \texttt{doc/doxygen/} and to the tutorials}

\TIP{The iterative solvers can directly be used for {\ublas}, Eigen and MTL4 objects! Please have a look at Chap.~\ref{chap:other-libs} and the respective tutorials in the examples/tutorials/ folder.}

\NOTE{In {\ViennaCLversion}, GMRES using ATI GPUs yields wrong results due to a bug in Stream SDK v2.1. Consider using newer versions of the Stream SDK.}

\begin{lstlisting}
viennacl::compressed_matrix<float>  vcl_matrix;
viennacl::vector<float>  vcl_rhs;
viennacl::vector<float>  vcl_result;

/* Set up matrix and vectors here */

//solution using conjugate gradient solver:
vcl_result = viennacl::linalg::solve(vcl_matrix,
             vcl_rhs,
             viennacl::linalg::cg_tag());

//solution using BiCGStab solver:
vcl_result = viennacl::linalg::solve(vcl_matrix,
             vcl_rhs,
             viennacl::linalg::bicgstab_tag());

//solution using GMRES solver:
vcl_result = viennacl::linalg::solve(vcl_matrix,
             vcl_rhs,
             viennacl::linalg::gmres_tag());
\end{lstlisting}

\begin{table}[tb]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{p{4cm}|p{3cm}|p{7.5cm}}
Method & Matrix class & ViennaCL\\
\hline
Conjugate Gradient (CG) & symmetric positive definite & \texttt{y = solve(A, x, cg\_tag());} \\
Stabilized Bi-CG (BiCGStab) & non-symmetric & \texttt{y = solve(A, x, bicgstab\_tag());} \\
Generalized Minimum Residual (GMRES) & general & \texttt{y = solve(A, x, gmres\_tag());} \\
\hline
\end{tabular}
\caption{Linear solver routines in {\ViennaCL} for the computation of $y$ in the expression $Ay = x$ with given $A$, $x$.}
\label{tab:linear-solvers}
\end{center}
\end{table}

Customized error tolerances can be set in the solver tags. The convention is
that solver tags take the relative error tolerance as first argument and the
maximum number of iteration steps as second argument. Furthermore, after the
solver run the number of iterations and the estimated error can be obtained from
the solver tags as follows:
\begin{lstlisting}
// conjugate gradient solver with tolerance 1e10
// and at most 100 iterations:
viennacl::linalg::cg_tag custom_cg(1e-10, 100);
vcl_result = viennacl::linalg::solve(vcl_matrix, vcl_rhs, custom_cg);
//print number of iterations taken and estimated error:
std::cout << "No. of iters: " << custom_cg.iters() << std::endl;
std::cout << "Est. error: " << custom_cg.error() << std::endl;
\end{lstlisting}
The BiCGStab solver tag can be customized in exactly the same way. The GMRES
solver tag takes as third argument the dimension of the Krylov space. Thus, a
tag for GMRES(30) with tolerance $1\mathrm{E}\!-\!10$ and at most $100$ total
iterations
(hence, up to three restarts) can be set up by
\begin{lstlisting}
viennacl::linalg::gmres_tag custom_gmres(1e-10, 100, 30);
\end{lstlisting}

\section{Preconditioners} \label{sec:preconditioner}
{\ViennaCL} ships with a generic implementation of several preconditioners.
The preconditioner setup is expect for simple diagonal preconditioners always carried out on the CPU host due to the need for dynamically allocating memory.
Thus, one may not obtain an overall performance benefit if too much time is spent on the preconditioner setup.

\TIP{The preconditioner also works for {\ublas} types!}

An overview of preconditioners available for the various sparse matrix types is as follows:
\begin{center}
 \begin{tabular}{|l|c|c|c|c|c|c|}
  \hline
  Matrix Type & ICHOL & (Block-)ILU[0/T] & Jacobi & Row-scaling & AMG & SPAI \\
  \hline
  \lstinline|compressed_matrix| & yes & yes & yes & yes & yes & yes \\
  \lstinline|coordinate_matrix| & no & no & yes & yes & no & no \\
  \lstinline|ell_matrix| & no & no & no & no & no & no \\
  \lstinline|hyb_matrix| & no & no & no & no & no & no \\
  \hline
 \end{tabular}
\end{center}
Broader support of preconditioners particularly for \lstinline|ell_matrix| and \lstinline|hyb_matrix| is scheduled for future releases.
AMG and SPAI preconditioners are described in Chap.~\ref{chap:additional-algorithms}.


In the following it is assumed that the sparse linear system of equations is given as follows:
\begin{lstlisting}
typedef viennacl::compressed_matrix<float>   SparseMatrix;

SparseMatrix  vcl_matrix;
viennacl::vector<float>  vcl_rhs;
viennacl::vector<float>  vcl_result;

/* Set up matrix and vectors here */
\end{lstlisting}

% \begin{table}[tb]
% \begin{center}
% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{p{3cm}|p{4cm}|p{7cm}}
% Method & Brief description & Parameters\\
% \hline
% ILUT & incomplete LU factorization & First parameter: Maximum number of entries
% per row. Second parameter: Drop tolerance. \\
% Jacobi & Divide each row in $A$ by its diagonal entry & none \\
% Row Scaling & Divide each row in $A$ by its norm & First parameter specifies
% the norm (1: $l^1$-norm, 2: $l^2$-norm)\\
% \hline
% \end{tabular}
% \caption{Preconditioners for iterative solvers in {\ViennaCL}.}
% \label{tab:preconditioners}
% \end{center}
% \end{table}

\subsection{Incomplete LU Factorization with Threshold (ILUT)}
The incomplete LU factorization preconditioner aims at computing sparse matrices lower and upper triangular matrices $L$ and $U$ such that the sparse system
matrix is approximately given by $A \approx LU$. In order to control the sparsity pattern of $L$ and $U$, a threshold strategy is used (ILUT)
\cite{saad-iterative-solution}. Due to the serial nature of the preconditioner, the setup of ILUT is always computed on
the CPU using the respective ViennaCL backend.

\begin{lstlisting}
//compute ILUT preconditioner:
viennacl::linalg::ilut_tag ilut_config;
viennacl::linalg::ilut_precond< SparseMatrix > vcl_ilut(vcl_matrix,
                                                        ilut_config);

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     vcl_ilut);   //preconditioner here
\end{lstlisting}
The triangular substitutions may be applied in parallel on GPUs by enabling \emph{level-scheduling} \cite{saad-iterative-solution} via the member function call \lstinline|use_level_scheduling(true)| in the \lstinline|ilut_config| object.

Three parameters can be passed to the constructor of \lstinline|ilut_tag|: The first specifies the maximum number of entries per row in $L$ and $U$, while the
second parameter specifies the drop tolerance. The third parameter is the boolean specifying whether level scheduling should be used.

\TIP{The performance of level scheduling depends strongly on the matrix pattern and is thus disabled by default.}

\subsection{Incomplete LU Factorization with Static Pattern (ILU0)}
Similar to ILUT, ILU0 computes an approximate LU factorization with sparse factors L and U.
While ILUT determines the location of nonzero entries on the fly, ILU0 uses the sparsity pattern of A for the sparsity pattern of L and U \cite{saad-iterative-solution}.
Due to the serial nature of the preconditioner, the setup of ILU0 is computed on the CPU.
\begin{lstlisting}
//compute ILU0 preconditioner:
viennacl::linalg::ilu0_tag ilu0_config;
viennacl::linalg::ilu0_precond< SparseMatrix > vcl_ilut(vcl_matrix,
                                                        ilu0_config);

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     vcl_ilut);   //preconditioner here
\end{lstlisting}
The triangular substitutions may be applied in parallel on GPUs by enabling \emph{level-scheduling} \cite{saad-iterative-solution} via the member function call \lstinline|use_level_scheduling(true)| in the \lstinline|ilu0_config| object.

One parameter can be passed to the constructor of \lstinline|ilu0_tag|, being the boolean specifying whether level scheduling should be used.

\TIP{The performance of level scheduling depends strongly on the matrix pattern and is thus disabled by default.}

\subsection{Block-ILU}
To overcome the serial nature of ILUT and ILU0 applied to the full system matrix,
a parallel variant is to apply ILU to diagonal blocks of the system matrix.
This is accomplished by the \lstinline|block_ilu| preconditioner, which takes
the system matrix type as first template argument and the respective ILU-tag type as second template argument
(either \lstinline|ilut_tag| or \lstinline|ilu0_tag|). Support for accelerators using {\CUDA} or {\OpenCL} is provided.

\begin{lstlisting}
//compute block-ILU preconditioner using ILU0 for each block:
block_ilu_precond<SparseMatrix,
                  ilu0_tag> vcl_block_ilu0(vcl_matrix,
                                           ilu0_tag());

//solve
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::bicgstab_tag(),
                                     vcl_block_ilu0);
\end{lstlisting}
A third argument can be passed to the constructor of \lstinline|block_ilu_precond|:
Either the number of blocks to be used (defaults to $8$), or an index vector with fine-grained control over the blocks. Refer to the Doxygen pages in doc/doxygen for details.

\TIP{The number of blocks is a design parameter for your sparse linear system at hand. Higher number of blocks leads to better memory bandwidth utilization on GPUs, but may increase the number of solver iterations.}

\subsection{Jacobi Preconditioner}
A Jacobi preconditioner is a simple diagonal preconditioner given by the reciprocals of the diagonal entries of the system matrix $A$.
Use the preconditioner as follows:
\begin{lstlisting}
//compute Jacobi preconditioner:
jacobi_precond< SparseMatrix > vcl_jacobi(vcl_matrix,
                                          viennacl::linalg::jacobi_tag());

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::cg_tag(),
                                     vcl_jacobi);
\end{lstlisting}


\subsection{Row Scaling Preconditioner}
A row scaling preconditioner is a simple diagonal preconditioner given by the reciprocals of the norms of the rows of the system matrix $A$.
Use the preconditioner as follows:
\begin{lstlisting}
//compute row scaling preconditioner:
row_scaling< SparseMatrix > vcl_row_scaling(vcl_matrix,
                                      viennacl::linalg::row_scaling_tag());

//solve (e.g. using conjugate gradient solver)
vcl_result = viennacl::linalg::solve(vcl_matrix,
                                     vcl_rhs,
                                     viennacl::linalg::cg_tag(),
                                     vcl_row_scaling);
\end{lstlisting}
The tag \lstinline|viennacl::linalg::row_scaling_tag()| can be supplied with a parameter denoting the norm to be used. A value of \lstinline|1| specifies the
$l^1$-norm, while a value of $2$ selects the $l^2$-norm (default).


\section{Eigenvalue Computations}
%{\ViennaCL}
Two algorithms for the computations of the eigenvalues of a matrix $A$ are implemented in {\ViennaCL}:
\begin{itemize}
\item The Power Iteration \cite{golub:matrix-computations}
\item The Lanczos Algorithm \cite{simon:lanczos-pro}
\end{itemize}
Depending on the parameter \lstinline|tag| either one of them is called.
Both algorithms can be used for either {\ublas} or {\ViennaCL} compressed matrices.\\
In order to get the eigenvalue with the greatest absolut value the power iteration should be called. \\
The Lanczos algorithm returns a vector of the largest eigenvalues with the same type as the entries of the matrix.

The algorithms are called for a matrix object \lstinline|A| by
\begin{lstlisting}
std::vector<double> largest_eigenvalues = viennacl::linalg::eig(A, ltag);
double largest_eigenvalue = viennacl::linalg::eig(A, ptag);
\end{lstlisting}


\subsection{Power Iteration}
The Power iteration aims at computing the eigenvalues of a matrix by calculating the product of the matrix and a vector for several times, where the resulting vector is used for the next product of the matrix and so on. The computation stops as soon as the norm of the vector converges. \\
The final vector is the eigenvector to the eigenvalue with the greatest absolut value.\\
To call this algorithm, \lstinline|piter_tag| must be used.
This tag has only one parameter: \\ \lstinline|terminationfactor| defines the accuracy of the computation, i.e. if the new norm of the eigenvector changes less than this parameter the computation stops and returns the corresponding eigenvalue (default: $1e-10$).\\
The call of the constructor may look like the following:
\begin{lstlisting}
viennacl::linalg::piter_tag ptag(1e-8);
\end{lstlisting}

\TIP{Example code can be found in \lstinline|examples/tutorial/power-iter.cpp|.}

\subsection{The Lanczos Algorithm}
In order to compute the eigenvalues of a sparse high-dimensional matrix the Lanczos algorithm can be used to find these.
This algorithm reformulates the given high-dimensional matrix in a way such that the matrix can be rewritten in a tridiagonal matrix at much lower dimension.
The eigenvalues of this tridiagonal matrix are equal to the largest eigenvalues of the original matrix. \\
The eigenvalues of the tridiagonal matrix are calculated by using the bisection method \cite{golub:matrix-computations}. \\
To call this Lanczos algorithm, \lstinline|lanczos_tag| must be used.
This tag has several parameters that can be passed to the constructor:

\begin{itemize}
 \item The exponent of epsilon for the tolerance of the reorthogonalization, defined by the parameter \lstinline|factor| (default: $0.75$)
 \item The method of the Lanczos algorithm: $0$ uses partial reorthogonalization, $1$ full reothogonalization and $2$ does not use reorthogonalization (default: $0$)
 \item The number of eigenvalues that are returned is specified by \lstinline|num_eigenvalues| (default: $10$)
 \item The size of the krylov space used for the computations can be set by the parameter \lstinline|krylov_size| (default: $100$). The maximum number of iterations can be equal or less this parameter
\end{itemize}
The call of the constructor may look like the following:
\begin{lstlisting}
viennacl::linalg::lanczos_tag ltag(0.85, 15, 0, 200);
\end{lstlisting}

\TIP{Example code can be found in \lstinline|examples/tutorial/lanczos.cpp|.}


\section{QR Factorization}

\NOTE{The current QR factorization implementation depends on {\ublas}.}

A matrix $A \in \mathbb{R}^{n\times m}$ can be factored into $A = Q R$, where $Q \in \mathbb{R}^{n\times n}$ is an
orthogonal matrix and $R \in \mathbb{R}^{n \times m}$ is upper triangular. This so-called QR-factorization is important for eigenvalue computations as well as
for the solution of least-squares problems \cite{golub:matrix-computations}. {\ViennaCL} provides a generic implementation of the QR-factorization using
Householder reflections in file \lstinline|viennacl/linalg/qr.hpp|. An example application can be found in \lstinline|examples/tutorial/qr.hpp|.

The Householder reflectors $v_i$ defining the Householder reflection $I - \beta_i v_i v_i^{\mathrm{T}}$ are stored in the
columns below the diagonal of the input matrix $A$ \cite{golub:matrix-computations}. The normalization coefficients $\beta_i$ are returned by the
worker function \lstinline|inplace_qr|. The upper triangular matrix $R$ is directly written to the upper triangular part of $A$.
\begin{lstlisting}
  std::vector<ScalarType> betas = viennacl::linalg::inplace_qr(A, 12);
\end{lstlisting}
If $A$ is a dense matrix from \ublas, the calculation is carried out on the CPU using a single thread. If $A$ is a
\lstinline|viennacl::matrix|, a hybrid implementation is used: The panel factorization is carried out using \ublas, while expensive BLAS level 3 operations
are computed on the OpenCL device using multiple threads.

Typically, the orthogonal matrix $Q$ is kept in inplicit form because of computational efficiency
However, if $Q$ and $R$ have to be computed explicitly, the function \lstinline|recoverQ| can be used:
\begin{lstlisting}
  viennacl::linalg::recoverQ(A, betas, Q, R);
\end{lstlisting}
Here, \lstinline|A| is the inplace QR-factored matrix, \lstinline|betas| are the coefficients of the Householder reflectors as returned by
\lstinline|inplace_qr|, while \lstinline|Q| and \lstinline|R| are the destination matrices. However, the explicit formation of $Q$ is expensive and is usually avoided.
For a number of applications of the QR factorization it is required to apply $Q^T$ to a vector $b$. This is accomplished by
\begin{lstlisting}
 viennacl::linalg::inplace_qr_apply_trans_Q(A, betas, b);
\end{lstlisting}
without setting up $Q$ (or $Q^T$) explicitly.

\TIP{Have a look at \lstinline|examples/tutorial/least-squares.cpp| for a least-squares computation using QR factorizations.}
